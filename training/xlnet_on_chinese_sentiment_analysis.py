# -*- coding: utf-8 -*-
"""XLNet on Chinese-Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SS14xagA52I4FEIpHMThkLqc90hDlDF2
"""

!pip install transformers
!pip install torch

!pip install simpletransformers

import transformers
from transformers import XLNetTokenizer, XLNetModel, AdamW, get_linear_schedule_with_warmup
import torch

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score 
from collections import defaultdict
from textwrap import wrap
from pylab import rcParams

from torch import nn, optim
from keras.preprocessing.sequence import pad_sequences
from torch.utils.data import TensorDataset,RandomSampler,SequentialSampler
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

# Import Dataset

import codecs
import csv
import os

import pandas as pd
import tensorflow as tf

URL = "https://paddlehub-dataset.bj.bcebos.com/chnsenticorp.tar.gz"


class DataLoader:
    def __init__(self):
        self.dataset_dir = os.path.join(".", "datasets/chnsenticorp")
        file_path = tf.keras.utils.get_file(
            fname="chnsenticorp.tar.gz",
            origin=URL,
            extract=True,
            cache_dir=".",
        )
        
        self.load_data()
    

    def load_data(self):
        self.train_file = os.path.join(self.dataset_dir, "train.tsv")
        self.train_df = self.get_df_from_file(self.train_file)
        self.train_num = len(self.train_df)

        self.dev_file = os.path.join(self.dataset_dir, "dev.tsv")
        self.dev_df = self.get_df_from_file(self.dev_file)
        self.dev_num = len(self.dev_df)

        self.test_file = os.path.join(self.dataset_dir, "test.tsv")
        self.test_df = self.get_df_from_file(self.test_file)
        self.test_df["labels"] = self.test_df["labels"].apply(lambda x: x[0])
        self.test_num = len(self.test_df)

    def get_train_df(self):
        return self.train_df

    def get_dev_df(self):
        return self.dev_df

    def get_test_df(self):
        return self.test_df

    def get_labels(self):
        return ["0", "1"]

    @property
    def num_labels(self):
        """
        Return the number of labels in the dataset.
        """
        return len(self.get_labels())

    def get_df_from_file(self, input_file, quotechar=None):
        with codecs.open(input_file, "r", encoding="UTF-8") as f:
            reader = csv.reader(f, delimiter="\t", quotechar=quotechar)
            data = []
            seq_id = 0
            header = next(reader)  # skip header
            for line in reader:
                data.append([line[1], line[0]])
                seq_id += 1
            df = pd.DataFrame(data)
            df.columns = ["text", "labels"]
            df["labels"] = df["labels"].apply(lambda x: list(map(int, x)))
            return df

    def print_info(self):
        print("training data:")
        print(self.get_train_df().head())
        print("\n")

        print("dev data:")
        print(self.get_dev_df().head())
        print("\n")
        
        print("test data:")
        print(self.get_test_df().head())
        print("\n")

        print("Train number:{}, Dev number:{}, Test number:{}".format(self.train_num, self.dev_num, self.test_num))


data = DataLoader()
data.print_info()

from simpletransformers.classification import ClassificationModel

# define hyperparameter
train_args ={"reprocess_input_data": True,
             "overwrite_output_dir": True,
             "fp16":False,
             "num_train_epochs": 4}

# Create a ClassificationModel
model = ClassificationModel(
    "xlnet", "hfl/chinese-xlnet-base",
    num_labels=data.num_labels,
    args=train_args
)

model.train_model(data.get_train_df())

import os
import tarfile

def save_model(model_path='',file_name=''):
  files = [files for root, dirs, files in os.walk(model_path)][0]
  with tarfile.open(file_name+ '.tar.gz', 'w:gz') as f:
    for file in files:
      f.add(f'{model_path}/{file}')

save_model('outputs','chinese-XLNet')

import os
import tarfile

def unpack_model(model_name=''): 
  tar = tarfile.open(f"{model_name}.tar.gz", "r:gz")
  tar.extractall()
  tar.close()

unpack_model('chinese-XLNet')

from simpletransformers.classification import ClassificationModel

# define hyperparameter
train_args ={"reprocess_input_data": True,
             "overwrite_output_dir": True,
             "fp16":False,
             "num_train_epochs": 4}

# Create a ClassificationModel
model = ClassificationModel(
    "xlnet", "outputs/",
    num_labels=data.num_labels,
    args=train_args
)

# Test the model
result, model_outputs, wrong_predictions =model.eval_model(data.get_test_df())
accuracy = (result['tp'] + result['tn'])/data.test_num
print(accuracy)

